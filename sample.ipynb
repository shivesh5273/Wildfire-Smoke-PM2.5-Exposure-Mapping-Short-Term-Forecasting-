{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Wildfire Smoke → PM2.5 Exposure Mapping + Short-Term Forecasting\n",
    "\n",
    "## Project goal\n",
    "Wildfire smoke can cause rapid increases in fine particulate matter (PM2.5), creating acute health risk.\n",
    "\n",
    "This project builds an end-to-end, reproducible ML pipeline to:\n",
    "1. **Estimate smoke-related PM2.5 exposure** (station-based + spatial mapping)\n",
    "2. **Forecast short-term PM2.5** (next-day / next-24h), using time-safe features\n",
    "3. **Quantify operational trade-offs** (Precision/Recall, threshold tuning, calibration)\n",
    "4. **Deliver deployable artifacts**: clean dataset, trained model, evaluation plots, and an exposure map product\n",
    "\n",
    "## Key deliverables\n",
    "- EDA + quality checks + missingness analysis\n",
    "- Feature engineering (lags, trends, meteorology, smoke flags, seasonality)\n",
    "- Train/validation/test with time-aware splits (no leakage)\n",
    "- Baselines + stronger models (linear + tree-based)\n",
    "- Probability calibration & threshold tuning for rare events (exceedance detection)\n",
    "- Exposure mapping: station values → gridded surface (simple interpolation for portfolio)\n",
    "- Clear limitations and next steps\n",
    "\n",
    "> NOTE: This notebook is designed to run in two modes:\n",
    "- **Mode A (recommended)**: you provide station PM2.5 CSV(s) and optionally smoke polygons.\n",
    "- **Mode B (optional)**: you enable downloads/API pulls (Open-Meteo for weather; NOAA HMS smoke polygons)."
   ],
   "id": "71593fc66eb74b12"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T23:45:42.823783Z",
     "start_time": "2026-01-19T23:45:35.180561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import math\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error, r2_score,\n",
    "    precision_recall_curve, average_precision_score,\n",
    "    precision_score, recall_score, f1_score, confusion_matrix,\n",
    "    brier_score_loss\n",
    ")\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams[\"figure.dpi\"] = 140"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data requirements (minimum viable)\n",
    "\n",
    "You need **station-level PM2.5** time series, ideally hourly. Minimum columns:\n",
    "\n",
    "- `datetime` (timestamp)\n",
    "- `lat`, `lon` (station location)\n",
    "- `pm25` (PM2.5 concentration, µg/m³)\n",
    "- `station_id` (unique id)\n",
    "\n",
    "Optional but recommended:\n",
    "- `state`, `county`, `site_name`\n",
    "- `aqi` or additional pollutants (PM10, O3)\n",
    "\n",
    "### Smoke signal options (choose one)\n",
    "**Option 1 (simple):** A daily smoke index column you provide per station (`smoke_flag` or `smoke_density`).\n",
    "**Option 2 (stronger):** NOAA HMS smoke polygons → spatial join to stations (we include a template).\n",
    "**Option 3 (portfolio-grade):** satellite AOD fields (optional later).\n",
    "\n",
    "### Meteorology (recommended)\n",
    "Meteorology improves signal separation between “local pollution” and “smoke-driven spikes”.\n",
    "We support:\n",
    "- **Open-Meteo API** (no key) for temperature, wind, humidity, precipitation, PBL height proxies (limited)\n",
    "- OR you load your own met CSV.\n",
    "\n",
    "In this notebook we treat meteorology as optional: models still run without it."
   ],
   "id": "4772bf4be59b42dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # Paths\n",
    "    DATA_DIR: Path = Path(\"data\")\n",
    "    OUT_DIR: Path = Path(\"outputs\")\n",
    "\n",
    "    # Input files (set these)\n",
    "    PM25_FILE: Path = Path(\"data/pm25_stations.csv\")  # REQUIRED\n",
    "    SMOKE_FILE: Optional[Path] = None                # optional: merged station-day smoke flags\n",
    "    MET_FILE: Optional[Path] = None                  # optional: merged station met data\n",
    "\n",
    "    # Forecast target setup\n",
    "    # Use daily aggregation for stability (you can change to hourly later).\n",
    "    AGG: str = \"D\"  # 'D' daily, 'H' hourly\n",
    "    FORECAST_HORIZON_STEPS: int = 1  # 1 day ahead if AGG='D'\n",
    "\n",
    "    # Exceedance threshold (for classification task)\n",
    "    EXCEED_PM25: float = 35.0  # µg/m³ (EPA 24-hr standard)\n",
    "\n",
    "    # Split dates (time-aware)\n",
    "    TRAIN_END: str = \"2021-12-31\"\n",
    "    VAL_END: str = \"2022-12-31\"   # test = after this\n",
    "    MIN_HISTORY_STEPS: int = 14   # require at least 2 weeks history per station for features\n",
    "\n",
    "cfg = Config()\n",
    "cfg.OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "cfg.DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cfg"
   ],
   "id": "705575a3cdd62ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 1 — Load & validate PM2.5 station data\n",
    "\n",
    "We first load station PM2.5 data and enforce:\n",
    "- consistent timestamps\n",
    "- numeric PM2.5\n",
    "- unique station identifiers\n",
    "- basic sanity bounds\n",
    "\n",
    "This is a “quality gate”: if you get bad data here, everything downstream becomes unreliable."
   ],
   "id": "e95bb04ffb13ea53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_pm25(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    # Standardize column names\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "\n",
    "    required = {\"datetime\", \"station_id\", \"lat\", \"lon\", \"pm25\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"PM2.5 file is missing columns: {missing}. Found: {df.columns.tolist()}\")\n",
    "\n",
    "    df[\"datetime\"] = pd.to_datetime(df[\"datetime\"], utc=True, errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"datetime\", \"station_id\", \"lat\", \"lon\", \"pm25\"]).copy()\n",
    "    df[\"pm25\"] = pd.to_numeric(df[\"pm25\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"pm25\"]).copy()\n",
    "\n",
    "    # Basic bounds (keep wide; smoke can be high)\n",
    "    df = df[(df[\"pm25\"] >= 0) & (df[\"pm25\"] <= 2000)].copy()\n",
    "\n",
    "    # Ensure types\n",
    "    df[\"station_id\"] = df[\"station_id\"].astype(str)\n",
    "    df[\"lat\"] = df[\"lat\"].astype(float)\n",
    "    df[\"lon\"] = df[\"lon\"].astype(float)\n",
    "\n",
    "    return df\n",
    "\n",
    "pm = load_pm25(cfg.PM25_FILE)\n",
    "pm.head(), pm.shape"
   ],
   "id": "da6c0dff65903b3e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 2 — Exploratory Data Analysis (EDA)\n",
    "\n",
    "We answer:\n",
    "- How many stations do we have?\n",
    "- How long is the time series?\n",
    "- Are there missing periods or sparse stations?\n",
    "- Are there obvious seasonal patterns?\n",
    "- Are extreme values plausible?\n",
    "\n",
    "EDA is not “pretty pictures” — it’s how we prevent silent failure.\n",
    "\n",
    "We will:\n",
    "1. Plot station coverage (count over time)\n",
    "2. Show distribution of PM2.5 (log + linear)\n",
    "3. Plot example station time series\n",
    "4. Summarize missingness after resampling"
   ],
   "id": "b2f1f46d94d70c2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Basic summary\n",
    "print(\"n_rows:\", len(pm))\n",
    "print(\"n_stations:\", pm[\"station_id\"].nunique())\n",
    "print(\"time range:\", pm[\"datetime\"].min(), \"→\", pm[\"datetime\"].max())\n",
    "\n",
    "# Station count over time (raw timestamps)\n",
    "pm_temp = pm.copy()\n",
    "pm_temp[\"date\"] = pm_temp[\"datetime\"].dt.floor(\"D\")\n",
    "station_count = pm_temp.groupby(\"date\")[\"station_id\"].nunique()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(station_count.index, station_count.values)\n",
    "plt.title(\"Active stations over time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"# Stations with data\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# PM2.5 distribution\n",
    "vals = pm[\"pm25\"].values\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.hist(vals, bins=80)\n",
    "plt.title(\"PM2.5 distribution (linear)\")\n",
    "plt.xlabel(\"PM2.5 (µg/m³)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.hist(np.log1p(vals), bins=80)\n",
    "plt.title(\"PM2.5 distribution (log1p)\")\n",
    "plt.xlabel(\"log(1 + PM2.5)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Example stations\n",
    "example_ids = pm[\"station_id\"].drop_duplicates().head(3).tolist()\n",
    "for sid in example_ids:\n",
    "    d = pm[pm[\"station_id\"] == sid].sort_values(\"datetime\").copy()\n",
    "    # downsample for plot speed\n",
    "    d = d.set_index(\"datetime\")[\"pm25\"].resample(\"D\").mean().reset_index()\n",
    "    plt.figure(figsize=(9,3))\n",
    "    plt.plot(d[\"datetime\"], d[\"pm25\"])\n",
    "    plt.title(f\"Example station {sid} — daily mean PM2.5\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"PM2.5 (µg/m³)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "id": "d90cd7d4fde700a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 3 — Daily aggregation + forecasting target\n",
    "\n",
    "Why daily?\n",
    "- Smoke health impact is often framed in 24-hour exposure.\n",
    "- Many stations have missing hourly points; daily aggregation stabilizes.\n",
    "- It makes forecasting and evaluation cleaner.\n",
    "\n",
    "We compute per station per day:\n",
    "- `pm25_mean` (daily mean)\n",
    "- `pm25_max` (daily max)\n",
    "- `exceed` = 1 if `pm25_mean >= 35` (24-hr standard default; adjustable)\n",
    "\n",
    "Forecast target:\n",
    "- `y_reg` = PM2.5 mean at **t + 1 day**\n",
    "- `y_cls` = exceedance at **t + 1 day**\n",
    "\n",
    "IMPORTANT: Features must only use information available at time t (time-safe)."
   ],
   "id": "53ce52b7f298664b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def aggregate_daily(pm: pd.DataFrame, agg: str = \"D\") -> pd.DataFrame:\n",
    "    df = pm.copy()\n",
    "    df = df.sort_values([\"station_id\", \"datetime\"])\n",
    "    df = df.set_index(\"datetime\")\n",
    "\n",
    "    # Station metadata (static)\n",
    "    meta = df.groupby(\"station_id\")[[\"lat\", \"lon\"]].first()\n",
    "\n",
    "    daily = (\n",
    "        df.groupby(\"station_id\")[\"pm25\"]\n",
    "          .resample(agg)\n",
    "          .agg(pm25_mean=\"mean\", pm25_max=\"max\", pm25_count=\"count\")\n",
    "          .reset_index()\n",
    "    )\n",
    "    daily = daily.merge(meta.reset_index(), on=\"station_id\", how=\"left\")\n",
    "    daily = daily.rename(columns={\"datetime\": \"date\"})\n",
    "    return daily\n",
    "\n",
    "daily = aggregate_daily(pm, cfg.AGG)\n",
    "daily.head(), daily.shape"
   ],
   "id": "cf91074fb52b945c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def add_targets(daily: pd.DataFrame, horizon_steps: int, exceed_thr: float) -> pd.DataFrame:\n",
    "    df = daily.sort_values([\"station_id\", \"date\"]).copy()\n",
    "    df[\"exceed_today\"] = (df[\"pm25_mean\"] >= exceed_thr).astype(int)\n",
    "\n",
    "    # Targets at t+h\n",
    "    df[\"y_reg\"] = df.groupby(\"station_id\")[\"pm25_mean\"].shift(-horizon_steps)\n",
    "    df[\"y_cls\"] = df.groupby(\"station_id\")[\"exceed_today\"].shift(-horizon_steps)\n",
    "\n",
    "    # Drop rows where target is unknown (tail)\n",
    "    df = df.dropna(subset=[\"y_reg\", \"y_cls\"]).copy()\n",
    "    df[\"y_cls\"] = df[\"y_cls\"].astype(int)\n",
    "    return df\n",
    "\n",
    "df0 = add_targets(daily, cfg.FORECAST_HORIZON_STEPS, cfg.EXCEED_PM25)\n",
    "df0.head(), df0.shape"
   ],
   "id": "fb32e57218f7796a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 4 — Feature engineering (time-safe)\n",
    "\n",
    "We build predictors that are available at time t:\n",
    "\n",
    "### A) History / momentum features\n",
    "- Lags: yesterday / 2 days / 4 days PM2.5\n",
    "- Rolling mean: last 4 days average\n",
    "- Deltas: change over last 1 day, 2 days, 4 days\n",
    "\n",
    "These capture persistence and sudden jumps (common with smoke intrusions).\n",
    "\n",
    "### B) Spatiotemporal context\n",
    "- latitude, longitude\n",
    "- day-of-year (seasonality)\n",
    "- weekday / month (optional)\n",
    "\n",
    "### C) Smoke signal (optional)\n",
    "- station-day `smoke_flag` (0/1) or `smoke_density`\n",
    "If provided, this is often the strongest feature.\n",
    "\n",
    "### D) Meteorology (optional)\n",
    "- wind speed / direction proxy\n",
    "- temperature\n",
    "- humidity\n",
    "- precipitation\n",
    "This helps separate smoke transport from local sources.\n",
    "\n",
    "We keep everything “leakage-safe”: no peeking into future days."
   ],
   "id": "b0b8edb0fc688582"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def make_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = df.sort_values([\"station_id\", \"date\"]).copy()\n",
    "\n",
    "    # Calendar\n",
    "    d[\"date_local\"] = pd.to_datetime(d[\"date\"]).dt.tz_convert(None) if str(d[\"date\"].dtype).startswith(\"datetime64\") else pd.to_datetime(d[\"date\"])\n",
    "    d[\"doy\"] = d[\"date_local\"].dt.dayofyear\n",
    "    d[\"month\"] = d[\"date_local\"].dt.month\n",
    "    d[\"dow\"] = d[\"date_local\"].dt.dayofweek\n",
    "\n",
    "    # Lags of pm25_mean\n",
    "    for k in [1, 2, 4, 7]:\n",
    "        d[f\"pm25_lag_{k}\"] = d.groupby(\"station_id\")[\"pm25_mean\"].shift(k)\n",
    "\n",
    "    # Rolling mean (past only)\n",
    "    d[\"pm25_rollmean_4\"] = (\n",
    "        d.groupby(\"station_id\")[\"pm25_mean\"]\n",
    "         .shift(1)\n",
    "         .rolling(window=4, min_periods=2)\n",
    "         .mean()\n",
    "         .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "    # Deltas (momentum)\n",
    "    d[\"dpm25_1\"] = d[\"pm25_lag_1\"] - d[\"pm25_lag_2\"]\n",
    "    d[\"dpm25_2\"] = d[\"pm25_lag_2\"] - d[\"pm25_lag_4\"]\n",
    "    d[\"dpm25_4\"] = d[\"pm25_lag_1\"] - d[\"pm25_lag_7\"]\n",
    "\n",
    "    return d\n",
    "\n",
    "df1 = make_features(df0)\n",
    "\n",
    "# Require history\n",
    "feature_cols_preview = [c for c in df1.columns if c.startswith(\"pm25_\") or c.startswith(\"dpm25_\")] + [\"lat\",\"lon\",\"doy\",\"month\",\"dow\"]\n",
    "df1[feature_cols_preview].isna().mean().sort_values(ascending=False).head(15)"
   ],
   "id": "9f3d1a3f056373f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 5 (optional) — Merge smoke + meteorology\n",
    "\n",
    "If you have these, merge them now (station_id + date):\n",
    "\n",
    "- Smoke: `smoke_flag` or `smoke_density`\n",
    "- Meteorology: `wind_speed`, `temperature`, `humidity`, `precip`, etc.\n",
    "\n",
    "If you *do not* have them yet, skip this section — the notebook still works.\n",
    "\n",
    "### Recommended practical approach (portfolio-friendly)\n",
    "- Start with PM2.5-only model (lags + seasonality).\n",
    "- Add smoke_flag later → show clear performance improvement.\n",
    "- Add meteorology after → demonstrate “scientific features” thinking."
   ],
   "id": "494e6ad4ed22dfbb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def merge_optional(df: pd.DataFrame, smoke_file: Optional[Path], met_file: Optional[Path]) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    if smoke_file is not None and Path(smoke_file).exists():\n",
    "        smoke = pd.read_csv(smoke_file)\n",
    "        smoke.columns = [c.strip().lower() for c in smoke.columns]\n",
    "        if \"date\" in smoke.columns:\n",
    "            smoke[\"date\"] = pd.to_datetime(smoke[\"date\"], utc=True, errors=\"coerce\")\n",
    "        if not {\"station_id\",\"date\"}.issubset(smoke.columns):\n",
    "            raise ValueError(\"SMOKE_FILE must contain station_id and date columns.\")\n",
    "        smoke[\"station_id\"] = smoke[\"station_id\"].astype(str)\n",
    "        out = out.merge(smoke, on=[\"station_id\",\"date\"], how=\"left\")\n",
    "        print(\"Merged smoke features:\", [c for c in smoke.columns if c not in [\"station_id\",\"date\"]])\n",
    "\n",
    "    if met_file is not None and Path(met_file).exists():\n",
    "        met = pd.read_csv(met_file)\n",
    "        met.columns = [c.strip().lower() for c in met.columns]\n",
    "        if \"date\" in met.columns:\n",
    "            met[\"date\"] = pd.to_datetime(met[\"date\"], utc=True, errors=\"coerce\")\n",
    "        if not {\"station_id\",\"date\"}.issubset(met.columns):\n",
    "            raise ValueError(\"MET_FILE must contain station_id and date columns.\")\n",
    "        met[\"station_id\"] = met[\"station_id\"].astype(str)\n",
    "        out = out.merge(met, on=[\"station_id\",\"date\"], how=\"left\")\n",
    "        print(\"Merged met features:\", [c for c in met.columns if c not in [\"station_id\",\"date\"]])\n",
    "\n",
    "    return out\n",
    "\n",
    "df2 = merge_optional(df1, cfg.SMOKE_FILE, cfg.MET_FILE)\n",
    "df2.shape"
   ],
   "id": "91ee2fb107e812e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 6 — Train/Validation/Test split (time-aware)\n",
    "\n",
    "We do **chronological splits** to simulate real forecasting:\n",
    "\n",
    "- Train: up to `TRAIN_END`\n",
    "- Validation: `TRAIN_END` → `VAL_END`\n",
    "- Test: after `VAL_END`\n",
    "\n",
    "Why this matters:\n",
    "- Random splits leak seasonality and smoke episodes across splits.\n",
    "- Time splits reflect real deployment: train on past, predict future."
   ],
   "id": "79a4be6a2427e974"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def time_split(df: pd.DataFrame, train_end: str, val_end: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    d = df.copy()\n",
    "    d[\"date\"] = pd.to_datetime(d[\"date\"], utc=True)\n",
    "    train_end = pd.to_datetime(train_end, utc=True)\n",
    "    val_end = pd.to_datetime(val_end, utc=True)\n",
    "\n",
    "    train = d[d[\"date\"] <= train_end].copy()\n",
    "    val = d[(d[\"date\"] > train_end) & (d[\"date\"] <= val_end)].copy()\n",
    "    test = d[d[\"date\"] > val_end].copy()\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "train_df, val_df, test_df = time_split(df2, cfg.TRAIN_END, cfg.VAL_END)\n",
    "\n",
    "print(train_df.shape, val_df.shape, test_df.shape)\n",
    "print(\"Train date range:\", train_df[\"date\"].min(), \"→\", train_df[\"date\"].max())\n",
    "print(\"Val date range:\", val_df[\"date\"].min(), \"→\", val_df[\"date\"].max())\n",
    "print(\"Test date range:\", test_df[\"date\"].min(), \"→\", test_df[\"date\"].max())"
   ],
   "id": "fff1839131583ce8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Choose feature columns (dynamic based on optional merges)\n",
    "base_features = [\n",
    "    \"lat\", \"lon\", \"doy\", \"month\", \"dow\",\n",
    "    \"pm25_mean\", \"pm25_max\",\n",
    "    \"pm25_lag_1\",\"pm25_lag_2\",\"pm25_lag_4\",\"pm25_lag_7\",\n",
    "    \"pm25_rollmean_4\",\n",
    "    \"dpm25_1\",\"dpm25_2\",\"dpm25_4\",\n",
    "    \"pm25_count\"\n",
    "]\n",
    "\n",
    "optional_features = []\n",
    "for c in df2.columns:\n",
    "    if c in [\"smoke_flag\",\"smoke_density\",\"smoke_cat\"]:\n",
    "        optional_features.append(c)\n",
    "    if c in [\"wind_speed\",\"wind_gust\",\"temperature\",\"humidity\",\"precip\",\"pressure\"]:\n",
    "        optional_features.append(c)\n",
    "\n",
    "cat_features = [\"station_id\"]  # station fixed effects (helps)\n",
    "num_features = [c for c in base_features + optional_features if c in df2.columns]\n",
    "\n",
    "target_reg = \"y_reg\"\n",
    "target_cls = \"y_cls\"\n",
    "\n",
    "print(\"Numeric features:\", num_features)\n",
    "print(\"Categorical features:\", cat_features)"
   ],
   "id": "f2f36f163de1dcc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 7 — Modeling strategy\n",
    "\n",
    "We run two tasks:\n",
    "\n",
    "### Task A: Regression (forecast PM2.5 level)\n",
    "Models:\n",
    "- **Ridge Regression** (strong baseline, interpretable)\n",
    "- **RandomForestRegressor** (nonlinear, robust)\n",
    "- **HistGradientBoostingRegressor** (strong tabular performance)\n",
    "\n",
    "Metrics:\n",
    "- MAE (primary)\n",
    "- RMSE\n",
    "- R²\n",
    "\n",
    "### Task B: Classification (forecast exceedance)\n",
    "We convert PM2.5 forecast into an operational decision:\n",
    "- `y_cls = 1` if next-day PM2.5 mean ≥ 35 µg/m³.\n",
    "\n",
    "Models:\n",
    "- Logistic Regression (with scaling)\n",
    "- Calibrated Logistic Regression (more reliable probabilities)\n",
    "\n",
    "Metrics:\n",
    "- PR-AUC (Average Precision) — best for rare events\n",
    "- Precision / Recall / F1\n",
    "- Confusion matrix\n",
    "- Threshold tuning to choose an operating point"
   ],
   "id": "7029a886e088382"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_features),\n",
    "        (\"cat\", categorical_transformer, cat_features),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")"
   ],
   "id": "ba75d536d67b3c2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def eval_regression(y_true, y_pred) -> Dict[str, float]:\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}\n",
    "\n",
    "X_train = train_df[num_features + cat_features]\n",
    "y_train = train_df[target_reg].astype(float)\n",
    "\n",
    "X_val = val_df[num_features + cat_features]\n",
    "y_val = val_df[target_reg].astype(float)\n",
    "\n",
    "X_test = test_df[num_features + cat_features]\n",
    "y_test = test_df[target_reg].astype(float)\n",
    "\n",
    "reg_models = {\n",
    "    \"Ridge\": Ridge(alpha=1.0, random_state=0),\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=300, random_state=0, n_jobs=-1, max_depth=None),\n",
    "    \"HistGB\": HistGradientBoostingRegressor(random_state=0, max_depth=6)\n",
    "}\n",
    "\n",
    "reg_results = []\n",
    "reg_fit = {}\n",
    "\n",
    "for name, model in reg_models.items():\n",
    "    pipe = Pipeline(steps=[(\"preprocess\", preprocess), (\"model\", model)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    pred_val = pipe.predict(X_val)\n",
    "    pred_test = pipe.predict(X_test)\n",
    "\n",
    "    res = {\"model\": name}\n",
    "    res.update({f\"VAL_{k}\": v for k, v in eval_regression(y_val, pred_val).items()})\n",
    "    res.update({f\"TEST_{k}\": v for k, v in eval_regression(y_test, pred_test).items()})\n",
    "    reg_results.append(res)\n",
    "    reg_fit[name] = pipe\n",
    "\n",
    "reg_table = pd.DataFrame(reg_results).sort_values(\"VAL_MAE\")\n",
    "reg_table"
   ],
   "id": "3a55a509d7572496"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Regression diagnostics\n",
    "\n",
    "We visualize:\n",
    "- predicted vs actual (test)\n",
    "- error distribution\n",
    "- a time slice for one station (forecast realism)\n",
    "\n",
    "These plots catch systematic bias (e.g., underpredicting high-smoke spikes)."
   ],
   "id": "c4422e20ecf4bf17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "best_reg_name = reg_table.iloc[0][\"model\"]\n",
    "best_reg = reg_fit[best_reg_name]\n",
    "print(\"Best regression model by VAL_MAE:\", best_reg_name)\n",
    "\n",
    "pred_test = best_reg.predict(X_test)\n",
    "\n",
    "# Pred vs actual\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(y_test, pred_test, s=8, alpha=0.4)\n",
    "mx = max(y_test.max(), pred_test.max())\n",
    "plt.plot([0, mx], [0, mx])\n",
    "plt.title(f\"Predicted vs Actual (TEST) — {best_reg_name}\")\n",
    "plt.xlabel(\"Actual PM2.5\")\n",
    "plt.ylabel(\"Predicted PM2.5\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Residuals\n",
    "resid = pred_test - y_test\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.hist(resid, bins=60)\n",
    "plt.title(\"Residual distribution (TEST)\")\n",
    "plt.xlabel(\"Prediction error (pred - actual)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Station slice\n",
    "sid = test_df[\"station_id\"].drop_duplicates().iloc[0]\n",
    "slice_df = test_df[test_df[\"station_id\"] == sid].sort_values(\"date\").copy()\n",
    "Xs = slice_df[num_features + cat_features]\n",
    "slice_df[\"pred_pm25\"] = best_reg.predict(Xs)\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(slice_df[\"date\"], slice_df[\"y_reg\"], label=\"Actual next-day PM2.5\")\n",
    "plt.plot(slice_df[\"date\"], slice_df[\"pred_pm25\"], label=\"Predicted next-day PM2.5\")\n",
    "plt.title(f\"Station slice — {sid} (TEST)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"PM2.5\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "fea78a56ef17de60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X_train_c = train_df[num_features + cat_features]\n",
    "y_train_c = train_df[target_cls].astype(int)\n",
    "\n",
    "X_val_c = val_df[num_features + cat_features]\n",
    "y_val_c = val_df[target_cls].astype(int)\n",
    "\n",
    "X_test_c = test_df[num_features + cat_features]\n",
    "y_test_c = test_df[target_cls].astype(int)\n",
    "\n",
    "base_clf = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", LogisticRegression(max_iter=2000, class_weight=\"balanced\", n_jobs=-1))\n",
    "])\n",
    "\n",
    "base_clf.fit(X_train_c, y_train_c)\n",
    "\n",
    "# Calibrated version (better probability estimates)\n",
    "cal_clf = CalibratedClassifierCV(\n",
    "    estimator=Pipeline(steps=[\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"model\", LogisticRegression(max_iter=2000, class_weight=\"balanced\", n_jobs=-1))\n",
    "    ]),\n",
    "    method=\"isotonic\",  # use \"sigmoid\" if isotonic overfits\n",
    "    cv=3\n",
    ")\n",
    "cal_clf.fit(X_train_c, y_train_c)\n",
    "\n",
    "def get_prob(model, X):\n",
    "    return model.predict_proba(X)[:, 1]\n",
    "\n",
    "val_prob_base = get_prob(base_clf, X_val_c)\n",
    "test_prob_base = get_prob(base_clf, X_test_c)\n",
    "\n",
    "val_prob_cal = get_prob(cal_clf, X_val_c)\n",
    "test_prob_cal = get_prob(cal_clf, X_test_c)\n",
    "\n",
    "ap_val_base = average_precision_score(y_val_c, val_prob_base)\n",
    "ap_test_base = average_precision_score(y_test_c, test_prob_base)\n",
    "\n",
    "ap_val_cal = average_precision_score(y_val_c, val_prob_cal)\n",
    "ap_test_cal = average_precision_score(y_test_c, test_prob_cal)\n",
    "\n",
    "print(\"AP (PR-AUC) base: VAL\", round(ap_val_base, 4), \"TEST\", round(ap_test_base, 4))\n",
    "print(\"AP (PR-AUC) cal : VAL\", round(ap_val_cal, 4), \"TEST\", round(ap_test_cal, 4))"
   ],
   "id": "3d1b70a4b2d86f4b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 8 — PR curve + Calibration\n",
    "\n",
    "### PR curve (Precision–Recall)\n",
    "For rare events, PR curve is the most informative:\n",
    "- Moving along the curve corresponds to different thresholds.\n",
    "- Higher recall usually means more false alarms (lower precision).\n",
    "\n",
    "### Calibration curve\n",
    "Calibration answers: “If the model says 30%, is it *actually* 30%?”\n",
    "This matters for decision-making and interpretability."
   ],
   "id": "1dd9838b1fa248d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_pr(y_true, probs, title):\n",
    "    prec, rec, thr = precision_recall_curve(y_true, probs)\n",
    "    ap = average_precision_score(y_true, probs)\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(rec, prec, label=f\"AP={ap:.3f}\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return prec, rec, thr, ap\n",
    "\n",
    "print(\"Calibrated model PR curves:\")\n",
    "prec_v, rec_v, thr_v, ap_v = plot_pr(y_val_c, val_prob_cal, \"PR Curve (Validation) — Calibrated\")\n",
    "prec_t, rec_t, thr_t, ap_t = plot_pr(y_test_c, test_prob_cal, \"PR Curve (Test) — Calibrated\")"
   ],
   "id": "aed5f0f68f97d279"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_calibration(y_true, probs, title):\n",
    "    frac_pos, mean_pred = calibration_curve(y_true, probs, n_bins=10, strategy=\"quantile\")\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(mean_pred, frac_pos, marker=\"o\")\n",
    "    plt.plot([0,1],[0,1])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Mean predicted probability\")\n",
    "    plt.ylabel(\"Fraction of positives\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "print(\"Brier score (lower is better):\", round(brier_score_loss(y_test_c, test_prob_cal), 4))\n",
    "plot_calibration(y_test_c, test_prob_cal, \"Calibration curve (TEST) — Calibrated\")"
   ],
   "id": "482fb820005ebaa4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 9 — Threshold tuning (operational choice)\n",
    "\n",
    "Default threshold 0.5 is rarely optimal for imbalanced events.\n",
    "\n",
    "We choose thresholds based on:\n",
    "- **Best F1** on validation (balanced screening rule)\n",
    "- **High recall** (early warning mode)\n",
    "- **Higher precision** (false-alarm control / triage mode)\n",
    "\n",
    "We then report VAL + TEST metrics for each threshold and show the confusion outcomes."
   ],
   "id": "25ce8c17c5811ae9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def metrics_at_threshold(y_true, probs, thr: float) -> Dict[str, float]:\n",
    "    y_pred = (probs >= thr).astype(int)\n",
    "    p = precision_score(y_true, y_pred, zero_division=0)\n",
    "    r = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f = f1_score(y_true, y_pred, zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred)  # [[TN, FP],[FN, TP]]\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    return {\n",
    "        \"precision\": p, \"recall\": r, \"f1\": f,\n",
    "        \"TN\": tn, \"FP\": fp, \"FN\": fn, \"TP\": tp,\n",
    "        \"alerts\": int(tp + fp)\n",
    "    }\n",
    "\n",
    "# 1) Best-F1 threshold on validation\n",
    "prec, rec, thr = precision_recall_curve(y_val_c, val_prob_cal)\n",
    "# thr length = len(prec)-1\n",
    "f1s = (2 * prec[:-1] * rec[:-1]) / (prec[:-1] + rec[:-1] + 1e-12)\n",
    "best_idx = np.argmax(f1s)\n",
    "thr_best_f1 = thr[best_idx]\n",
    "\n",
    "# 2) High recall target\n",
    "target_recall = 0.80\n",
    "idx_recall = np.where(rec[:-1] >= target_recall)[0]\n",
    "thr_high_recall = thr[idx_recall[-1]] if len(idx_recall) else None\n",
    "\n",
    "# 3) Higher precision target\n",
    "target_precision = 0.20\n",
    "idx_prec = np.where(prec[:-1] >= target_precision)[0]\n",
    "thr_high_prec = thr[idx_prec[0]] if len(idx_prec) else None\n",
    "\n",
    "thresholds = {\n",
    "    \"Best-F1 (VAL)\": float(thr_best_f1),\n",
    "    \"High-Recall (VAL)\": float(thr_high_recall) if thr_high_recall is not None else np.nan,\n",
    "    \"High-Precision (VAL)\": float(thr_high_prec) if thr_high_prec is not None else np.nan\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for name, tval in thresholds.items():\n",
    "    if not np.isfinite(tval):\n",
    "        continue\n",
    "    val_m = metrics_at_threshold(y_val_c, val_prob_cal, tval)\n",
    "    test_m = metrics_at_threshold(y_test_c, test_prob_cal, tval)\n",
    "    rows.append({\n",
    "        \"choice\": name,\n",
    "        \"threshold\": tval,\n",
    "        \"VAL_precision\": val_m[\"precision\"],\n",
    "        \"VAL_recall\": val_m[\"recall\"],\n",
    "        \"VAL_f1\": val_m[\"f1\"],\n",
    "        \"TEST_precision\": test_m[\"precision\"],\n",
    "        \"TEST_recall\": test_m[\"recall\"],\n",
    "        \"TEST_f1\": test_m[\"f1\"],\n",
    "        \"TEST_FP\": test_m[\"FP\"],\n",
    "        \"TEST_TP\": test_m[\"TP\"],\n",
    "        \"TEST_alerts\": test_m[\"alerts\"],\n",
    "    })\n",
    "\n",
    "comparison = pd.DataFrame(rows).sort_values(\"TEST_f1\", ascending=False)\n",
    "comparison.style.format({\n",
    "    \"threshold\": \"{:.6f}\",\n",
    "    \"VAL_precision\": \"{:.3f}\",\n",
    "    \"VAL_recall\": \"{:.3f}\",\n",
    "    \"VAL_f1\": \"{:.3f}\",\n",
    "    \"TEST_precision\": \"{:.3f}\",\n",
    "    \"TEST_recall\": \"{:.3f}\",\n",
    "    \"TEST_f1\": \"{:.3f}\",\n",
    "})"
   ],
   "id": "7341adc4296bde03"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 10 — Operating threshold (locked)\n",
    "\n",
    "We choose **one default threshold** to make the project operationally complete.\n",
    "\n",
    "Recommendation:\n",
    "- **Best-F1 threshold** as the default “balanced screening rule”\n",
    "\n",
    "Operational alternatives:\n",
    "- Use **High-Recall** for early warning (accept high alert volume)\n",
    "- Use **High-Precision** for triage efficiency (fewer alerts, more misses)"
   ],
   "id": "39d1ba1792aa5ade"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "LOCKED_THRESHOLD = thresholds[\"Best-F1 (VAL)\"]\n",
    "LOCKED_THRESHOLD"
   ],
   "id": "5b8d6e059f266cd4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 11 — Exposure mapping (simple gridding)\n",
    "\n",
    "To visualize smoke exposure geographically, we convert station PM2.5 into a grid.\n",
    "\n",
    "This is not a full atmospheric dispersion model. It’s a **data product**:\n",
    "- take station PM2.5 for a chosen day\n",
    "- interpolate to a lat/lon grid using inverse-distance weighting (IDW)\n",
    "\n",
    "This delivers a clear “NASA-style” map output for a portfolio.\n",
    "\n",
    "If you later add smoke polygons / satellite AOD, you can make this stronger."
   ],
   "id": "c8a0725f77341a4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def idw_interpolate(xy: np.ndarray, values: np.ndarray, grid_xy: np.ndarray, power: float = 2.0, eps: float = 1e-12):\n",
    "    # xy shape (n,2), grid_xy shape (m,2)\n",
    "    out = np.zeros(grid_xy.shape[0], dtype=float)\n",
    "    for i, g in enumerate(grid_xy):\n",
    "        d = np.sqrt(((xy - g) ** 2).sum(axis=1)) + eps\n",
    "        w = 1.0 / (d ** power)\n",
    "        out[i] = np.sum(w * values) / np.sum(w)\n",
    "    return out\n",
    "\n",
    "# Pick a day from TEST for map demo\n",
    "map_day = test_df[\"date\"].sort_values().iloc[-30]  # ~one month before end\n",
    "day_df = daily.copy()\n",
    "day_df[\"date\"] = pd.to_datetime(day_df[\"date\"], utc=True)\n",
    "day_slice = day_df[day_df[\"date\"] == map_day].dropna(subset=[\"pm25_mean\"]).copy()\n",
    "\n",
    "print(\"Map day:\", map_day)\n",
    "print(\"Stations on day:\", len(day_slice))\n",
    "\n",
    "# Grid bounds\n",
    "lat_min, lat_max = day_slice[\"lat\"].min(), day_slice[\"lat\"].max()\n",
    "lon_min, lon_max = day_slice[\"lon\"].min(), day_slice[\"lon\"].max()\n",
    "\n",
    "# Coarse grid for speed (increase resolution later)\n",
    "n_lat, n_lon = 60, 80\n",
    "grid_lats = np.linspace(lat_min, lat_max, n_lat)\n",
    "grid_lons = np.linspace(lon_min, lon_max, n_lon)\n",
    "grid_xy = np.array([(la, lo) for la in grid_lats for lo in grid_lons])\n",
    "\n",
    "xy = day_slice[[\"lat\",\"lon\"]].values\n",
    "vals = day_slice[\"pm25_mean\"].values\n",
    "\n",
    "grid_vals = idw_interpolate(xy, vals, grid_xy, power=2.0)\n",
    "grid_vals = grid_vals.reshape(n_lat, n_lon)\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "plt.imshow(\n",
    "    grid_vals,\n",
    "    origin=\"lower\",\n",
    "    extent=[lon_min, lon_max, lat_min, lat_max],\n",
    "    aspect=\"auto\"\n",
    ")\n",
    "plt.scatter(day_slice[\"lon\"], day_slice[\"lat\"], s=10, alpha=0.7)\n",
    "plt.title(f\"Estimated PM2.5 exposure surface (IDW) — {map_day.date()}\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.colorbar(label=\"PM2.5 (µg/m³)\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ],
   "id": "8d1b09710060a228"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Results Summary (Final)\n",
    "\n",
    "### What worked\n",
    "- Time-safe lag features capture persistence and rapid changes.\n",
    "- PR-AUC provides the correct view for rare exceedance prediction.\n",
    "- Threshold tuning turns a model score into an operational decision rule.\n",
    "- Exposure mapping produces a portfolio-ready visual product.\n",
    "\n",
    "### What to report in GitHub / LinkedIn\n",
    "- Best regression model and MAE/RMSE on TEST\n",
    "- Classification PR-AUC and threshold table\n",
    "- The locked operating threshold + the two operational alternatives\n",
    "- A map example day showing exposure surface\n",
    "\n",
    "### Limitations\n",
    "- Station coverage is uneven; interpolation is not a physical dispersion model.\n",
    "- Smoke signal is stronger when you add NOAA HMS smoke polygons / AOD.\n",
    "- Meteorology improves generalization and should be added for production.\n",
    "\n",
    "### Next steps (future iteration, optional)\n",
    "- Add NOAA HMS smoke polygons → station smoke density feature\n",
    "- Add AOD (satellite) or wind trajectory features\n",
    "- Cross-validation grouped by geography + time\n",
    "- Deploy a simple CLI/predict script for next-day alerts"
   ],
   "id": "1750a7e48a6115a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bb46191f169de605"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
